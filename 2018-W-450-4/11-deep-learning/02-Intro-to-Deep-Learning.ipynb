{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Biological Neuron\n",
    "---\n",
    "Brains are able to accomplish a variety of tasks that give their host organism a leg up on the competition. At a behavioral level, they coordante a host of tasks for animals, including finding food, avoiding predation, reproduction, and heat regulation. All these tasks involve the input of light, sound, chemical, pressure, and heat sensors (i.e. eyes, ears, nose/mouth, skin), integrating and processing these signals, and ultimately executing some kind of action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Brains are made up of specialized cells of two classes: neurons and glia. Neurons are responsible for most of the information traffic in the brain. Humans have about **86 billion** neurons and $~1.5x10^{14}$ synapses. A synapse is an electrochemical connection between two neurons. Over time, the strengths, or **weights**, of these connections change. This is called **plasticity**, which is responsible for higher-level functions like learning, memory, and habits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./assets/biological_neuron.png' alt=\"biological neuron\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When it comes to information processing, individual neurons have a few anatomical structures of note:\n",
    "\n",
    "* **Dendrites** and **dendritic branches** revieve information from other neurons. Each neuron revieves inputs from many (up to thousands) of 'upstream' neurons. Because each of the synapses have different weights, the neuron 'listens' mostly to the neurons with the strongest input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **The axon hillock** sums the inputs of all the dendrites and makes a decision whether the neuron shoudl fire or not fire or not fire. If the sum of inputs is great enough, the neuron decides to fire, and sends an action potention traveling down the **axon**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Synaptic terminals** 'listen' for action potentials traveling down the axon. If they detect a large enough change in voltage, they will release neurotransmitters, which are neurons' means for communication wich each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Artificial Neuron\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first artificial neuron was conceived in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts (The McCulloch and Pitts Neuron). This neuron receives binary inputs, sums the inputs, calculates whether that sum is over some threshold, and generates a binary output. These types of neurons can perform logical operations like **and, or, not, ** and **xor**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The perceptron (Frank Rosenblatt, 1957), illustrated below, is based off another type of artificial neuron, called the **linear threshold unit**, or **LTU**, where inputs are both continuous. Inputs are **weighted**, **summed**, and applied to a **step function**, as seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./assets/perceptron.png' alt=\"artificial neuron\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inputs are $X_1$, $X_2$, and $X_3$, weights (think $\\beta$s or coefficients) are $W_1$, $W_2$ and $W_3$, and the sum of the inputs * weights is $z$.\n",
    "\n",
    "Some common step functions for perceptrons include:\n",
    "\n",
    "$$\n",
    "\\text{heaviside }(z) = \\begin{cases}\n",
    "      0, & \\text{if}\\ z < 0 \\\\\n",
    "      1, & \\text{if}\\ z \\geq 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{sgn }(z) = \\begin{cases}\n",
    "      -1, & \\text{if}\\ z < 0 \\\\\n",
    "      0, & \\text{if}\\ z = 0 \\\\\n",
    "      1, & \\text{if}\\ z > 0\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "/poll \"Does the perceptron remind you of any models we've learned?\" \"Linear models\" \"Tree-based models\" \"Neighbor-based models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptrons do not need to have just 1 output, they may have many. Here is another example of a perceptron. Although the architechture is slightly different (i.e. included a bias term, only 2 inputs, multiple outputs) all the rules from above hold true.\n",
    "\n",
    "<img src='./assets/perceptron2.png' alt=\"artificial neuron\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "/poll \"What type of operation is the above DAG representing?\" \"Regression\" \"Classification\" \"Clustering\"\n",
    "\n",
    "/poll \"How many features are there in thie data?\" \"1\" \"2\" \"3\"\n",
    "\n",
    "/poll \"If this DAG represents a classification, how many classes could there be?\" \"1\" \"2\" \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron weights are updated by the following rule:\n",
    "\n",
    "$$\n",
    "w_{i,j}^{\\text{(next step)}} = w_{i,j} + \\eta(\\hat{y}_j - y_j)x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where...\n",
    "* $w_{i,j}$ is the connection weight between the $i^{\\text{th}}$ input and the $j^{\\text{th}}$ neuron\n",
    "* $x_i$ is the $i^{\\text{th}}$ value of the current training instance (think $i^{\\text{th}}$ feature)\n",
    "* $\\hat{y}_j$  is the output of the $j^{\\text{th}}$ output neuron for the current training instance.\n",
    "* $y_j$ is the target output of the $j^{\\text{th}}$ output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "/poll \"We've seen this update rule before! Where does it come from?\" \"Gradient Boosting\" \"SVM\" \"Gradient Descent\" \"Nearest Neighbors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Biological Neural Networks\n",
    "---\n",
    "Neurons in the mammalian neocortex, or the outermost portion of your brain, is organized in a **laminar** fashion. All this means is that it's made up of layers. A cortical column is a collection of neurons that spans a layer. This basic structure is repeated all throughout the cortex, and consists of inputs from other brain regions, intermediate processing steps, and outputs. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./assets/layers.png' alt=\"layers\" style=\"width: 700px;\"/>\n",
    "\n",
    "Biological neural networks have infrastructure in place for feedback to correct 'errors' of 'upstream' neurons. This results in a framework where \"cells that fire together wire together\" - Siegrid Löwel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deeper Artificial Neural Networks (ANNs) and Backpropogation\n",
    "---\n",
    "This is a slightly more complicated artificial neural network with multiple layers (deep), a **multi-layer perceptron**. The most challenging aspect of deepening our neural networks is updating the weights. The error of our $y$ only applies to the very last layer. How do we update the rest of the weights?\n",
    "\n",
    "<img src='./assets/mlp.png' alt=\"layers\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropogation:\n",
    "\n",
    "For each training instance, the algorithm feeds it to the network and computes the output of every neuron in each consecutive layer (this is the forward pass, just like when making predictions). Then it measures the network’s output error (i.e., the difference between the desired output and the actual output of the network), and it computes how much each neuron in the last hidden layer contributed to each output neuron’s error. It then proceeds to measure how much of these error contributions came from each neuron in the previous hidden layer, and so on until the algorithm reaches the input layer. This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (hence the name of the algorithm). The last step of the backpropagation algorithm is a Gradient Descent step on all the connection weights in the network, using the error gradients measured earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luckily for us, the core of deep learning frameworks (ex: TensorFlow, Theano, Torch, Caffe, H2O, Chainer, etc.) is the **autodiff** algorithm, that automatically computes gradients for each layer for us. All we need to do is pick an **optimizer** to update our weights to reduce our error!\n",
    "\n",
    "[Additional reading on backprop from scratch](http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
    "\n",
    "[Andreij Karpathy](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Other activation functions:\n",
    "\n",
    "For artificial neurons to generate different types of outputs, we can swap out the step function for a different type of **activation function**. **Activation functions** take the sum of weighted inputs and produce some output. Here are a few examples of step functions and their derivatives. Some of them may even look familiar:\n",
    "\n",
    "<img src='./assets/activation_functions.png' alt=\"activation functions\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example of an MLP that with a **softmax** activation layer on top. Can you guess what this MLP does (classification or regression)?\n",
    "\n",
    "<img src='./assets/mlpsoftmax.png' alt=\"activation functions\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions / Discussion:\n",
    "\n",
    "/poll \"Is this MLP performing classification or regression?\" \"Classification\" \"Regression\"\n",
    "\n",
    "Slack question: What is the difference in terms of the prediction/output between the previous activation functions (step functions) and the softmax function?\n",
    "\n",
    "Slack question: What are some applications we could use an MLP for?\n",
    "\n",
    "/poll \"How many output neurons do we need to perform a regression with an MLP?\" \"1\" \"2\" \"3\"\n",
    "\n",
    "/poll \"Do you need an activation function for regression?\" \"yes\" \"no\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning in Vision\n",
    "---\n",
    "In the early study of vision, David Hubel and Torsten Wiesel discovered that neurons in primary visual cortex have different **receptive fields**. This means that neurons in visual cortex will pay attention to different orientations, spatial frequencies, and locations in within the visual field. \n",
    "\n",
    "<img src='./assets/visual_cortex.png' style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As information moves through the mammalian visual system, higher-order brain regions represent more abstract concepts. For example, V1, the first region of cortex that receives visual input, is sensitive to lines. V2 receives input from V1 and is thought to encode texture. By the time visual information reaches V4, color is also encoded. In the pathway that is responsible for object recognition, visual information arrives in the medial temoporal lobe, where sets of neurons encode representations of objects.\n",
    "\n",
    "This functional organization is mirrored by convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "---\n",
    "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data, and the tricks (like backpropogation) for training deep nets, CNNs have managed to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at other tasks, such as voice recognition or natural language processing (NLP); however, we will focus on visual applications for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Convolutional Layer\n",
    "\n",
    "In perceptrons, each neuron pays attention to an individual feature. To work with images, images would have to be flattened into one long vector of pixel values. The **convolutional layer** allows each neuron to pay attention to multiple pixels at the same time, so we no longer have the need to flatten images. How do convolutional layers do accomplish this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='./assets/conv1.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Notice that each neuron in the first convolutional layer pays attention to a specific region of the input image. You can think of each neuron as having a 'flashlight' that represents its receptive field. That neuron only pays attention to the information under its flashlight beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Zero Padding\n",
    "\n",
    "If the 'flashlight' moves by one for each convolutional 'neuron', the subsequent layer is not the same size as the current. For example, if you have a 5x5 image and have a 2x2 flashlight, your next layer will only be 4x4. This is why it is common to 'pad' inputs to a convolutional layer with zeros, as seen below:\n",
    "\n",
    "<img src='./assets/padding.png' style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Stride\n",
    "\n",
    "Adjuasting the stride of convolutions can help reduce dimensionality. Immagine if the 'flashlight' moved by 2 instead of 1- this would reduce the ammount of neurons you need in the next layer. Here is an example:\n",
    "\n",
    "<img src='./assets/stride.png' style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Filters\n",
    "\n",
    "A neuron’s weights can be represented as a small image the size of the receptive field. For example, the figure below shows two possible sets of weights, called  lters (or convolution kernels). The first one is represented as a black square with a vertical white line in the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of 1s); neurons using these weights will ignore everything in their receptive field except for the central vertical line (since all inputs will get multiplied by 0, except for the ones located in the central vertical line). The second filter is a black square with a horizontal white line in the middle. Once again, neurons using these weights will ignore everything in their receptive field except for the central horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in the figure below, the layer will output the top-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. Similarly, the upper-right image is what you get if all neurons use the horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter gives you a feature map, which highlights the areas in an image that are most similar to the filter. During training, a CNN finds the most useful filters for its task, and it learns to combine them into more complex patterns (e.g., a cross is an area in an image where both the vertical filter and the horizontal filter are active)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./assets/filters.png' style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Up to now, for simplicity, we have represented each convolutional layer as a thin 2D layer, but in reality it is composed of several feature maps of equal sizes, so it is more accurately represented in 3D. Within one feature map, all neurons share the same parameters (weights and bias term), but different feature maps may have different parameters. A neuron’s receptive field is the same as described earlier, but it extends across all the previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple filters to its inputs, making it capable of detecting multiple features anywhere in its inputs. Moreover, input images are also composed of multiple sublayers: one per color chan‐ nel. There are typically three: red, green, and blue (RGB). Grayscale images have just one channel, but some images may have much more—for example, satellite images that capture extra light frequencies (such as infrared)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='./assets/filter_stacking.png' style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aside: Pooling\n",
    "\n",
    "Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting). Reducing the input image size also makes the neural network tolerate a little bit of image shift (location invariance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a max pooling layer (2 × 2 pooling kernel, stride 2, no padding):\n",
    "\n",
    "<img src='./assets/pooling.png' style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Typical CNN Architechture\n",
    "\n",
    "<img src='./assets/typical_cnn.png' style=\"width: 600px;\"/>\n",
    "\n",
    "This is a common architechture for CNNs, and one we will be implementing here. There are other architectures, and many of them come pre-trained! Here are some of their architechtures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LeNet- Yann LeCun, 1998\n",
    "<img src='./assets/lenet.png' style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AlexNet- Alex Krizhevsky, 2012\n",
    "<img src='./assets/alexnet.png' style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GoogLeNet- Christian Szegedy, 2014\n",
    "<img src='./assets/googlenet.png' style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Retraining only a few layers ofa  pre-traind network is called **transfer learning**. This allows you to use powerful models trained by companies like Google for your own personal use! \n",
    "\n",
    "[Blog post on transfer learning](https://medium.com/towards-data-science/transfer-learning-using-keras-d804b2e04ef8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tomorrow we will go ahead and train our own convolutional neural net!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
