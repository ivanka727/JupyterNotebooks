{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/2018-W-450-4/06-hyper-parameter-tuning\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...............\n",
      "Solving package specifications: .\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /opt/conda:\n",
      "#\n",
      "tqdm                      4.19.6                     py_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda install tqdm --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train_df = data['adult']['train']['engineered']\n",
    "adult_train_target = data['adult']['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function_call(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = function(*args, **kwargs)\n",
    "        execution_time = time() - start\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "@time_function_call\n",
    "def model_fit(model, X, y):\n",
    "    return model.fit(X, y)\n",
    "\n",
    "@time_function_call\n",
    "def model_predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def run_model(model, model_name, data, labels):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels)\n",
    "    \n",
    "    _, fit_time = model_fit(model, X_train, y_train)\n",
    "    \n",
    "    train_pred, train_pred_time = model_predict(model, X_train)\n",
    "    \n",
    "    test_pred, test_pred_time = model_predict(model, X_test)   \n",
    "    \n",
    "    return {\n",
    "            'model' : model,\n",
    "            'model_name' : model_name,\n",
    "            'f1_train_score' : f1_score(y_train, train_pred),\n",
    "            'f1_test_score' : f1_score(y_test, test_pred),\n",
    "            'accuracy_train_score' : model.score(X_train, y_train),\n",
    "            'accuracy_test_score' : model.score(X_test, y_test),\n",
    "            'fit_time' : fit_time,\n",
    "            'train_pred_time' : train_pred_time,\n",
    "            'test_pred_time' : test_pred_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F$_1$ Score by Penalty Type and C Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.91s/it]\n"
     ]
    }
   ],
   "source": [
    "test_results = []\n",
    "k_range = range(1, 5)\n",
    "\n",
    "for k in tqdm(k_values):\n",
    "    model = KNeighborsClassifier(n_neighbors = 5)\n",
    "    test_result = run_model(model, \n",
    "                            regularization_type, \n",
    "                            adult_train_df, \n",
    "                            adult_train_target)\n",
    "    test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_test_score</th>\n",
       "      <th>accuracy_train_score</th>\n",
       "      <th>f1_test_score</th>\n",
       "      <th>f1_train_score</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_pred_time</th>\n",
       "      <th>train_pred_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840619</td>\n",
       "      <td>0.888954</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.755504</td>\n",
       "      <td>0.585056</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>l2</td>\n",
       "      <td>3.593916</td>\n",
       "      <td>10.981462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_test_score  accuracy_train_score  f1_test_score  f1_train_score  \\\n",
       "0             0.840619              0.888954       0.648363        0.755504   \n",
       "\n",
       "   fit_time                                              model model_name  \\\n",
       "0  0.585056  KNeighborsClassifier(algorithm='auto', leaf_si...         l2   \n",
       "\n",
       "   test_pred_time  train_pred_time  \n",
       "0        3.593916        10.981462  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60, 70, 80, 90]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(range(10, 100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_results_df['max_depth'] = test_results_df.model.apply(lambda model: model.max_depth) # decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "for regularization_type in ['l1', 'l2']:\n",
    "    for c in tqdm(c_values):\n",
    "        model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "        test_result = run_model(model, \n",
    "                                regularization_type, \n",
    "                                adult_train_df, \n",
    "                                adult_train_target)\n",
    "        test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "for regularization_type in ['l1', 'l2']:\n",
    "    for c in tqdm(c_values):\n",
    "        model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "        test_result = run_model(model, \n",
    "                                regularization_type, \n",
    "                                adult_train_df, \n",
    "                                adult_train_target)\n",
    "        test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "\n",
    "for _ in range(10):\n",
    "    for regularization_type in ['l1', 'l2']:\n",
    "        for c in tqdm(c_values):\n",
    "            model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "            test_result = run_model(model, \n",
    "                                    regularization_type, \n",
    "                                    adult_train_df, \n",
    "                                    adult_train_target)\n",
    "            test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import viridis_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(viridis_r.colors)/12)\n",
    "colors = [col for i, col in enumerate(viridis_r.colors) if i % 21 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(20,12))\n",
    "for i, C in enumerate(l1_test_results.C.unique()): \n",
    "    l1_test_results_for_C = l1_test_results[l1_test_results.C == C]\n",
    "    sns.distplot(l1_test_results_for_C.f1_test_score, label=str(C), ax=ax1, color=colors[i])\n",
    "    l2_test_results_for_C = l2_test_results[l2_test_results.C == C]\n",
    "    sns.distplot(l2_test_results_for_C.f1_test_score, label=str(C), ax=ax2, color=colors[i])\n",
    "ax1.set_xlim(0.6,0.7)\n",
    "ax1.set_title('Distribution of L1 penalty performance')\n",
    "ax1.legend()\n",
    "ax2.set_xlim(0.6,0.7)\n",
    "ax2.set_title('Distribution of L2 penalty performance')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
